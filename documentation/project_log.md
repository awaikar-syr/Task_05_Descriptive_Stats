# Project Log - Task 05: Descriptive Statistics and LLMs

## Project Start: December 2024

### Phase 1: Setup and Dataset Selection

#### Repository Setup ✅
- Created organized folder structure
- Set up comprehensive README.md
- Established documentation framework
- Created data processing script template
- Set up prompt collection framework
- Created project tracking TODO list
- Set up .gitignore for dataset exclusion
- Created requirements.txt with dependencies
- Initial commit completed

#### Dataset Selection ✅
- **Selected Dataset:** Syracuse Women's Basketball 2023-24
- **Source:** User provided CSV file
- **Dataset Characteristics:**
  - 11 players, 32 games
  - 22 statistical columns per player
  - Rich basketball statistics (scoring, shooting, rebounds, assists, etc.)
  - Perfect size for LLM testing (small but comprehensive)
  - Season record: 24-8 (13-5 ACC), #20 AP ranking, NCAA tournament

#### Data Analysis Setup ✅
- Created specialized basketball analyzer script
- Generated baseline statistics and visualizations
- Created basketball-specific prompt collection
- Established validation framework for LLM responses

#### Next Steps:
1. Test LLMs with basic questions
2. Document LLM responses and accuracy
3. Progress to intermediate and advanced questions
4. Develop prompt engineering strategies

---

## Research Questions to Explore

### Basic Questions (Easy for LLMs)
- How many games did the team play?
- What was the team's win-loss record?
- Who scored the most points/goals?
- What was the average score per game?

### Intermediate Questions (Require Analysis)
- Who was the most improved player?
- Which games were the closest/most lopsided?
- What was the team's performance trend over the season?
- Who were the most consistent performers?

### Advanced Questions (Complex Analysis)
- As a coach, should I focus on offense or defense to win 2 more games?
- Which player should I work with to be a game changer?
- What factors contributed most to wins vs losses?
- How would the season have been different if we had won specific close games?

---

## LLM Testing Strategy

### LLMs to Test:
1. **ChatGPT** (GPT-4)
2. **Claude** (Anthropic)
3. **Co-Pilot** (GitHub)

### Testing Approach:
1. Start with basic questions
2. Document initial accuracy
3. Refine prompts based on results
4. Progress to complex questions
5. Validate all responses against actual data

---

## Key Insights and Learnings
*[To be populated as project progresses]*

---

## Challenges Encountered
*[To be populated as project progresses]*

---

## Next Reporting Date: July 31st 